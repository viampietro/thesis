In this section, we present the review of the work done in the
literature pertaining to transformation functions with a focus in the
context of formal verification. Here, a transformation function is
understood as any kind of mapping from a source representation to a
target representation, where the source and target representation
possess a behavior of their own (i.e, they are
executable). Especially, we are interested in two things:

\begin{enumerate}
\item Is there a proper way to build a transformation function? Do
  standards exist to do this depending on the application domain? How
  can we build a modular, extensible transformation function? How can
  we build a transformation function that will ease the proof of
  semantic preservation?
\item In the context of formal verification, how are expressed the
  semantic preservation theorems? Are there usual proof strategies?
\end{enumerate}

Here, the goal is to inspire ourselves with the work of the
literature, and to see how far the correspondence holds between our
specific case of transformation, and other cases of transformations.
The results of the literature review are presented in two parts. The
two parts have been prepared based on the same material. The first
part will be focusing on the expression of the transformation
functions in the literature, and the second part will be focusing on
the proof that these transformations are semantic preserving ones.

The material we used for the literature review is divided in three
categories. Each category covers a specific case of transformation
function, always taken in the context of formal verification. Thus,
the three categories are:

\begin{itemize}
\item Compilers for generic programming languages
\item Compilers for hardware description languages
\item Model-to-model and model-to-text transformations
\end{itemize}

\subsection{Building transformation functions}
\label{sec:build-transf}

As the authors state in \cite{Tan2016}, ``Although theoretically
possible, verifying a compiler that is not designed for verification
would be a prohibitive amount of work in practice.'' The question is
to know how to design such a compiler? How to anticipate the fact that
we will have to prove that the compiler is semantic preserving? We
open these to the more general context of transformation functions
that map a source representation to target one.

\paragraph{Compilers for generic programming languages} In the context
of formally verified compilers for generic programming languages,
translation from a source program to a target program is mostly
straight forward. While descending recursively through the AST of the
input program, each construct of the source language is mapped to one
or many constructs of the target
language. Figure~\ref{fig:java-expr-to-java-bytecode} gives an example
of the translation from \java{} program expressions to \java{}
bytecode expressions, set in the context of a compiler for \java{}
programs written within the \isahol{} theorem prover
\cite{Strecker2002}. Here, the mapping between source and target
constructs is clearly defined.
\begin{figure}[H]
  \centering
  \includegraphics[keepaspectratio,width=.6\linewidth]{Figures/Proof/java-exprs-transl}
  \caption{Translation from \java{} expressions to \java{} bytecode expressions}
  \label{fig:java-expr-to-java-bytecode}
\end{figure}
In the works pertaining to the well-known \ccert project
\cite{Leroy2009, Blazy2006}, the many passes building the compiler
from C programs to assembly languages are also clearly mapping each
construct of source program to target program constructs. This is all
the more natural, since the languages like \coq, \isa, \hol and other
interactive theorem provers permit to perform pattern matching on the
abstract syntax tree of the source programs.

The cases of optimizing compilers, like \cite{Leroy2009} and
\cite{Tan}, show that to avoid to write too complex functions when
passing from source to target program, that would too difficult to
handle during the semantic preservation proof, the compilation is
decomposed into many passes. No more than 12 passes for the CakeML
compiler, and up to 7 passes for \ccert. This is a way to keep
translation functions simple in order to ease reasoning
afterwards. Indeed, the more the gap is important between the source
representation and the target one, the more the translation function
will be complex.

Another point that is noticeable is the expression of translation
function is the necessity to keep a binding between source and target
representations. For instance, in \ccert, when passing from
transformed C programs to an RTL representation (based on registers
and control flow graphs), a binding function $\gamma$ links the
variables found in C programs to the registers generated in the RTL
representation. The binding is important for the translation
(replacing variables by their corresponding registers in the RTL
code), and for the proof when values from the source program will be
compared to values in the target program. This is a thing that should
be anticipated when writing a translation function, i.e what is the
correspondence between source and target elements.

In \cite{Leroy2009}, and \cite{Chlipala2010}, compilers are written
within the \coq{} proof assistant. Compilers are expressed using the
state-and-error monad, thus mimicking the traits of imperative
languages into a functional programming language setting.

\paragraph{Compilers for hardware description languages}

The other category of compilers that we are interested in are
compilers for hardware description languages (HDL). The \hilecop{}
methodology's goal is the design of harware circuits. For that reason,
we are interested in studying the case of compilers for HDLs. However,
one should notice that compiling a HDL program into a lower level
representation is one level of abstraction down compared to the
transformation we propose to verify. Indeed, it corresponds to step 3
in the \hilecop{} methodology, i.e the translation from VHDL to RTL
representation.

In the context of formal verification applied to HDLs compilers, only
a few works describe the specificities of their translation function.

In \cite{Braibant2013}, the authors describe the definition of the
language FeSi (a refinement of the BlueSpec language, a specification
language for hardware circuit behaviors), and its embedding in \coq{}.
The authors present the syntax and semantics of the FeSi and of the
RTL language which is the target language of the compiler.  FeSi
programs are composed of simple expressions, and actions permitting to
read or write from different types of memory (registers). Therefore,
the abstract syntax is divided into the definition of expressions and
the definition of actions, i.e: control flow instructions and
operations on memory. The RTL language is composed of expressions and
write operations to registers. The authors are more interested in
proving that a FeSi specification is well-implemented by a given
\coq{} program, than giving the details of the translation from FeSi
to RTL. However, the translation seems straight-forward, and proceeds
as usual by descending through the AST of FeSi programs.

In \cite{Bourgeat2020}, the authors present a compiler for the
language Koîka, which is also a simplification of the BlueSpec
language. A Koîka program is composed of a list of rules; each rule
describes actions that must be performed atomically. Actions are read
and write operations on registers. A Koîka program is accompanied by a
scheduler that specify an execution order for the rules. The described
compiler transforms Koîka programs into RTL descriptions of hardware
circuits. The translation function builds an RTL circuit by descending
recursively down the AST of rules. Each action is translated into a
specific RTL representation which are afterwards composed together to
get complex circuits. The translation becomes trickier when it comes
to decide the composition of RTL circuits to respect the execution
order prescribed by the scheduler.

In \cite{Bourke}, the authors present the verification of a compiler
toolchain from Lustre programs to an imperative language (Obc), and
from Obc to Clight.  The Clight target is the one defined in
\ccert{}\cite{Leroy2009}.  Lustre permits the definition of programs
composed of nodes that are executed synchronously.  Nodes treat input
streams and yield output streams of values.  A node body is composed
of sequence of equations that determine the values of output streams
based on the input.  Obc programs are composed of class
declarations. A class declaration has a vector of memory variables, a
vector of instances of other classes, and method declarations.  The
translation turns each node of a Lustre program into a class of Obc
having two methods: reset, for the initialization of the streams, and
step, for the update of values resulting of a synchronous step.

In \cite{Loow2021}, the authors describe a compiler that transforms
Verilog programs into netlists targetting certain FPGA models. Verilog
programs are a lot like VHDL programs; they describe a hardware
circuit behavior in terms of processes. A netlist is composed of
registers, variables and a list of cells corresponding to
combinational components. During the translation process, the
expressions of the Verilog programs are turned into netlist cells, and
the composition of statements leads to the creation of complex
circuits by means of cell composition.

\paragraph{Model-to-model and model-to-text transformations}

We are now presenting the works pertaining to model-to-model and
model-to-text transformations in the context of formal
verification. Because of the very nature of the transformation we
propose to verify, i.e a model-to-text transformation in the
\hilecop{} methodology, the following works are of particular interest
to us. We will focus here on the manner to express transformations in
the case of model-to-model and model-to-text transformations. Also, we
tried to find articles related to model transformations involving
Petri nets.

In \cite{Berramla2015}, the authors observe that Model-Driven
Engineering (MDE) is all about model transformation operations. They
propose to set a formal context within the \coq{} proof assistant to
verify that model transformations preserve the structure of the source
models into the target models. To illustrate their methodology, they
choose to transform UML state machine diagrams into Petri net
models. The translation rules from source to target models are
expressed within the setting of the OMG standard QVT language
(Query/View/Transform). The QVT language offers a formal way to
express model transformations, partly based on the Object Constraint
Language (OCL). The translation rules maps the different kind of
structures that can be found in UML state diagrams to specific
structures of Petri nets. Even though the two models used as source
and target of transformations are executable, the authors leverage the
formal context provided by \coq{} to prove that the expressed
transformations preserve certain structural properties.

In \cite{Calegari2011}, the authors describe a process for model
transformation where transformation rules are expressed with the Atlas
Transformation Language (ATL). Transformation rules in ATL involve
both declarative (OCL) and imperative (match rules) instructions. The
authors show how the ATL rules can easily be translated into \coq{}
relations. An example is given on the kind of model-to-model
transformations that can be implemented that way. The example is a UML
class diagram to relational database model transformation.

In \cite{Combemale2009}, the authors explore the different ways to
give a formal semantics to a Domain-Specific Language (DSL) in the
context of MDE. Consequently, the DSL is expressed with a meta-model.
An instantiation of this meta-model (a model) yields a DSL
program. The authors specify a transformation from a DSL model to
another executable model. While giving an operational semantics to the
DSL models, the aim of the transformation is to be able to compare the
execution of the target models with the execution of the source
models. The authors illustrate their approach with a source DSL named
xSPEM, a process description language, and the model domain is timed
PNs. The translation is expressed through a structural mapping; i.e,
each element of an xSPEM model is mapped to a particular PN: an
activity is mapped to a subnet, a resource to a single place,
connection from activity to resource through parameter is mapped to a
connection of transitions and places in the resulting PN\dots

In \cite{Dyck2019}, the authors address the problem of expressing
model transformations by using transformation graphs. Precisely, the
kind of transformation graphs that are used are called Triple Graph
Grammar (TGG). A TGG is a triplet ${<}s,c,t{>}$ where the
``correspondence model $c$ explicitly stores correspondence
relationships between source model $s$ and target model $t$''.

The work described is \cite{Fronc2011} is really close to our own
verification task. The article describes how Coloured Petri Nets
(CPNs, specifically LLVM-labelled Petri nets) are transformed into
LLVM programs representing the state space (the graph of reachable
markings) of these PNs.  The aim is to enable an efficient
model-checking of the CPNs.  LLVM-labelled PNs are CPNs whose places,
transitions and arcs have LLVM constructs for colour domains. Places
are labelled with data types.  Transitions are labelled with boolean
expressions, that correspond to the guard of the transition. Arcs are
labelled by multisets of expressions. A marking is a function that
maps each place to a multiset of values belonging to the place's type.
The authors define data structures (multisets, sets, markings,...)
with interfaces, i.e sets of operations over structures, to represent
the Petri nets in LLVM.  They define interpretation functions that
draw equivalences between Petri nets objects and LLVM data structures.
The authors define two algorithms: $\mathtt{fire\_t}$ and
$\mathtt{succ\_t}$ to compute the graph of reachable states.  These
are the functions that transform CPNs into concrete LLVM programs. 

In \cite{Meghzili2017}, the author describe a transformation from UML
state machine diagrams to Coloured Petri Nets (CPNs). The aim is to
leverage the means of analysis provided by Petri nets to certify
certain properties over UML state machine diagrams. The authors want
to verify that the transformation preserve structural properties
between source and target models. The transformation function does not
use a standard setting as QVT or ATL, or transformation graphs. It is
expressed as a specific function written in \isahol. 

In \cite{Yang2014}, the author present a transformation from
Architecture Analysis and Design Language (AADL) models to Timed
Abstract State Machines (TASMs). AADL is a language widely used in
avionics to describe both hardware and software systems. AADL doesn't
have a lot of tools to analyze and simulate the designed systems;
therefore transforming AADL models into TASM enables the use of an
important toolbox for analysis, and simulation. The transformation
from AADL to TASMs are described with ATL rules.

\paragraph{Discussions on how to build transformation functions in the
  context of semantic preservation}

Transformation functions are mappings from a source representation to
a target representation. The more the mapping from source to target is
straight-forward the easier the comparison will be when proving that
the transformation is semantic preserving. Thus, in
\cite{Leroy2009,Tan,Chlipala2010} where complex case of optimizing
compilers are presented, the compilation is split into many simple
pass to ease the verification effort coming afterwards.

Also, while translating source programs, the compiler must often
generate fresh constructs belonging to the target language (for
instance, generating an fresh RTL register for each variable
referenced in the source C program in \cite{Leroy2009}). The compiler
must keep a binding, that is, a memory of the mapping between the
elements of the source program and their mirror is the target
program. This consideration is of interest in our case of
transformation where the elements of SITPNs are also mirrored by
elements in the generated \hvhdl{} design.

It remains hard to establish a standard way to express a
transformation function as it really depends of the form of the input
and the output representation. Compilers for programming languages
tend to be a lot more compositional than model
transformations. Compositional meaning that the translation rules can
be split into simple and independent cases of translation, e.g
translation of expressions, then translation of statements, then
translation of function bodies,\dots In the world of models, there
exist some standard formalisms to express transformation rules (QVT,
ATL, transformation graphs\dots).  However, the complexity of the
transformation rules depends on the richness of the elements composing
the source model, and the distance to the concepts of the target
model.

\subsection{Transformations and proofs of semantic preservation }
\label{sec:lit-rev-trans-and-proof}

In this section, we are interested in how to prove that transformation
functions are semantic preserving. Especially, we are interested in
the expression of semantic preservation theorems, i.e, what does one
mean by semantic preservation, and in seeking usual proof strategies.

In the introduction of his article about \ccert{} \cite{Leroy2009},
X.Leroy presents the two points of major importance to express
semantic preservation theorems for GPL compilers, and more generally
to get the meaning of semantic preservation.

The first point is to clearly state how things are compared between
the source and the target programs. It is to describe the runtime
state of the source and the target, and to draw a correspondence
between two. This is expressed through a state comparison
relation.

The second point is to relate the execution of the source program to
the execution of the target program through a simulation
diagram. Figure shows the different kind of simulation diagrams
possibly relating two programs. Choosing an adequate simulation
diagram to express a semantic preservation theorem depends on the kind
of possible behaviors that can exhibit a given program. In the case of
GPL programs, X.Leroy lists three kinds of possible behaviors: either
the program execution succeeds and returns a value, or the program
execution fails and returns an error, or the program execution
diverges.

\begin{figure}[H]
  \centering
  \includegraphics[keepaspectratio,width=.6\linewidth]{Figures/Proof/sim-diagrams}
  \caption[Simulation diagrams]{Simulation diagrams between source and
    target programs}
  \label{fig:sim-diagrams}
\end{figure}

Anyway, in the case where the source program execution succeeds, the
theorem of semantic preservation takes this general form:

Consider a source program $P_1$ compiled into a target program $P_2$,
a starting state $S_1$ for program $P_1$ and a starting state $S_2$
for program $P_2$ such that $S_1$ and $S_2$ are similar states w.r.t.
the exhibited state comparison relation. If the execution of $P_1$
leads from state $S_1$ to state $S_1'$, then there exists a state
$S_2'$ resulting of the execution of program $P_2$ from state $S_2$
such that $S_1'$ and $S_2'$ are similar w.r.t. the exhibited state
comparison relation.

Compiler verification tasks aims at proving the kind of theorem stated
above. The other kind of task that can be applied to certify a
compiler is to perform compiler validation. Compiler validation is
interested in generating a proof of behavior preservation (or a
counter-example showing that behaviors diverge) for a given input
program alongside the compilation process. Thus, for a given input
program, the compiler yields a target program and the proof that the
input and target have the same behavior. Exhibiting a theorem of
semantic preservation is stronger than building a proof of semantic
preservation for each input program. Therefore, compiler verification
is stronger than compiler validation. The aim of the thesis is to
perform compiler \emph{verification} over the \hilecop{} methodology.
Some of the works, cited afterwards, are more interested in compiler
or transformation validation techniques than in verification. They are
presented here for the sake of coverage.

Now that we have clarified the meaning of semantic preservation for
GPL compilers, we state that this definition of semantic preservation
holds also for more general case of transformation from a source
representation to a target representation. The only condition to be
able to verify that a transformation is semantic preserving is that
the source and target representation must have an execution semantics
(i.e, the instances of the source and target representations must be
executable).

For each article used in the literature review and presenting a
specific case of transformation, the following questions have been
asked:

\begin{itemize}
\item What are the similarities/differences between source and target
  representations?
\item How are described the runtime state for the source and target
  representations?
\item How is expressed the state comparison relation?
\item How is stated the semantic preservation theorem?
\item What is the employed proof strategy?
\end{itemize}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../main"
%%% End:
