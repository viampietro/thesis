\chapter{Introduction}
\label{chap:intro}

With the use of every human-bred technology is associated a
risk. Regarding the nature of the technology, and the broader system
in which it is involved, the consequences of a failure can be
dramatic. Thus arises the notion of the safety of systems;
\cite{Bowen1993} gives the following definition of safety:

\begin{center}
  ``Safety can [\dots] be defined as the freedom from exposure to
  danger, or the exemption from hurt, injury or loss.''
\end{center}

A \textit{safety-critical} system can be understood as a system for
which the safety aspect is the main concern, being that important
consequences, such as direct human losses, natural catastrophes, or
economic disasters, could result of the failure of the system. In this
thesis, conducted in the field of computer science, we are
particularly interested in safety-critical \textit{computer}
systems. The concept of computer system encompasses both the low-level
hardware-related and the more abstract software-related aspects
involved in computer technologies. These days, computers pervade a
considerable number of objects and technologies that pave our
every-day life, including safety-critical systems. Thus, the risk
associated with the use of computers in certain critical applications
is real. Failures of safety-critical computer systems have happened
and continue to happen; the list of critical incidents maintained by
the ACM Committee on Computers and Public Policy and Peter G. Neumann
ever since the mid 80s \cite{Neumann1994} is always
growing\footnote{The \emph{risks digest} website continues to register
  the computer-related incidents that resulted or could result in
  important damages: \url{https://catless.ncl.ac.uk/Risks/} }.

To ensure the safety of computer systems involved in critical domains
such as avionics, railway, power plants or medicine, there exists a
number of standards and norms developed by international
organizations. These standards set of a number of rules and techniques
to be followed for the design, the production and the validation of
safety-critical computer systems. To cite some well-known standards,
the \href{https://www.eurocae.net/}{EUROCAE} and
\href{https://www.rtca.org/}{RTCA} organisms has devised the
ED-12C/DO-178C and ED-80/DO-254 industry standards for the development
cycle of software and hardware computer systems involved in avionics;
the \href{https://www.cencenelec.eu/}{CENELEC} has defined the
EN-50128 standard for the development of software programs for railway
control and protection systems; the \href{https://www.iec.ch}{IEC} is
at the origin of the IEC-60880 standard for the development of
software programs involved in the control of power plants; the USA,
Canada and the EU have defined the
\href{https://www.commoncriteriaportal.org/}{Common Criteria (CC)}
referential for the evaluation of the safety of systems and software
programs. In this thesis, we are interested in verifying a
\textit{computer-related} methodology involved in the production of
safety-critical medical devices. This domain is regulated by the EU
2017/145
standard\footnote{\url{http://data.europa.eu/eli/reg/2017/745/2020-04-24}}
that sets the rules for the development of medical devices, including
how to validate the technologies involved in the production line.

The rules imposed by the standards vary with respect to the
criticality of the considered systems; for instance, in the medical
field, the regulation text 2017/145 of the EU, pertaining to the
marketing of medical devices, sets a different requirement level
whether we are considering the production of dressings (level 0), or
of neuroprostheses (highest-level, level 4). The IEC defines a SIL
(Safety Integrity Level) measure that qualifies the criticality level
of a system. The CC defines a level of Evaluation Assurance Level
(EAL), from 1 to 7, that must be met by the evaluated systems
regarding their functional requirements.

Among the mandatory procedures, prescribed by the standards, which
must be followed to validate a computer system involved in a
safety-critical system, there are tests (unit, functional or
integration tests) or simulation (especially applied to hardware
systems) are to be noted.  However, in the case of the development
safety-critical computer systems, a particular kind of methods, called
\textit{formal methods} (FM), are also applied. In formal methods, a
computer system is considered as a mathematical object
\cite{Bjorner2014}. As pointed out in \cite{Bowen1993}, ``formal
methods address \textit{correctness} issues'', that is whether a
system delivers the required service. The perks of formal methods are
to set a formal mathematical framework around a computer system.  This
framework allows us to reason about the system and prove that the
system meets some required properties. Thus, a ``formal methods''
framework for computer systems requires at least a formal requirement
language, i.e. with a formal semantics, to express the properties that
a given system must verify. The expression of these required
properties is called the \textit{specification} of the system.  We can
cite some specification formalism such as CCS \cite{Milner1980}, CSP
\cite{Hoare1978}, Petri nets \cite{Petri1962} or TLA+
\cite{Lamport1994} to describe reactive systems (i.e. systems that
continuously interact with an environment); hardware description
languages such as \vhdl{} \cite{Lipsett1986} and \textsf{Verilog}
\cite{IEEE1996} can also be considered as formal specification
languages for hardware designs if provided with a formal semantics and
embedded in a formal proof system (see for instance
\cite{Borrione1992}). We can also cite specification languages such as
VDM \cite{Bjorner1987a}, the Z notation \cite{Abrial2013} on which is
based the B language \cite{Abrial1991} (included in the broader
B-method); but also, all the theorem provers and proof assistants that
come with their own specification languages such as \isahol{}
\cite{Nipkow2002}, \coq{} \cite{Coq2021}, \textsf{PVS}
\cite{Crow1995}, etc.  A FM framework must also provide a formal proof
system to reason about the formal specification of the system. Some FM
frameworks come with means to implement the computer system or
simplified version of the system (i.e. a model) in a formal
setting. This latter kind of framework enables to check if the
implementation of a system always complies with its specification,
i.e. the \textit{soundness} of the system, and if all the aspects of
the specification are met by the implementation, i.e. the
\textit{completeness} of the system.

Even though the purpose is always to check the correctness of systems,
there exist multiple kinds of formal verification techniques. These
techniques can be separated in three groups. The first group refers to
the \textit{deductive verification} methods; the techniques aims at
establishing some proofs over a computer system in a formal proof
setting; the deduction process can either be \textit{interactive}
(i.e. conducted by a human) or automated.  The second group refers to
model-checking techniques. The third group refers to the abstract
interpretation of programs. The techniques applied to the formal
verification of hardware computer systems or software computer systems
are quite similar.  Thus, most of the techniques presented here apply
to both hardware and software developments. Note that the following
presentation of formal verification techniques is not exhaustive.

\paragraph{Abstract interpretation}

Abstract interpretation \cite{Cousot1977} aims at performing automatic
static analysis of programs by approximating the possible execution
states of the program. To do so, the concrete domains of the program
variables are related to more abstract domains notably through the use
of a lattice structure. Afterwards, invariant properties that the
program verifies can be automatically checked against the lattice
structure. For instance, abstract interpretation can help to determine
that a given program terminates. To give examples of
\textit{abstract-interpretation-based} tools, we can cite the
TERMINATOR static analyzer \cite{Cook2006} that proves termination and
other properties over C programs, or the Astr√©e \cite{Blanchet2002}
program analyzer for real-time embedded systems.

Similarly to abstract interpretation, symbolic execution
\cite{Boyer1975} is another method for the static analysis of programs
based on the partial execution of the considered programs. The method
consists in generating all the execution traces, also called
\textit{symbolic} traces, of a given program; the result takes the
form of a symbolic execution tree. In these traces, some of the
program inputs, i.e. the \textit{variables}, will be associated with
\textit{symbolic} expressions denoting the fact that the values of
these inputs are yet unknown. Thus, by reasoning on the definition
domains of these inputs, some properties of the program can be checked
at execution points, that is, at the nodes of the symbolic execution
tree. The property checking process is most of the time performed by a
constraint solver \cite{Amadini2020}, where constraints are expressed
over the \textit{symbolic} variables of the program. Pertaining to the
construction of the symbolic execution tree, it is obtained most of
the time by applying the rules of a structural formal semantics
\cite{Plotkin2004} associated with the language of the considered
program. Typically, a branching in the execution tree is the result of
the evaluation of a conditional statement. Each path of the execution
tree is associated with a satisfiability condition, i.e. a Boolean
formula, that determines if a given execution point is reachable or
not. Symbolic execution methods are often used to generate test suites
with an important coverage of the execution paths of programs. To give
examples of \textit{symbolic-execution-based} tools, we can cite the
DART \cite{Godefroid2005} or CUTE \cite{Sen2006} test generators.

\paragraph{Model checking}
Model-checking techniques \cite{Queille1982, Clarke1986} build a model
reflecting the execution of a computer system by enumerating all the
possible execution states of the system. Then, the \textit{execution}
model can be automatically checked against some properties that the
computer system must verify. The execution model must be a
finite-state model, i.e. the enumeration of the execution states of
the model must not be infinite.  Most of the time, the properties that
the computer system must verify are expressed through formulas of a
modal logic.  Model-checking techniques are broadly used for the
formal verification of reactive systems, especially hardware
systems. In that case, the properties that must be met by the
considered system are expressed within formulas of one of the many
temporal logics (Interval Temporal Logic \cite{Moszkowski1985}, Linear
Temporal Logic \cite{Pnueli1977}, Computation Tree Logic
\cite{Emerson1984}, etc.), which are handy to express time-related
properties. To give examples of well-known model-checkers, we can cite
the BLAST \cite{Henzinger2002}, CADP \cite{Garavel2013} or
\textsc{Uppaal} \cite{Bengtsson1996} model-checkers.

\paragraph{Deductive software verification}
``Deductive software verification aims at formally verifying that all
possible behaviors of a given program satisfy formally defined,
possibly complex properties, where the verification process is based
on some form of logical inference, i.e., `deduction' ''
\cite{Hahnle2019}.  Deductive software verification methods are
divided into two categories: interactive theorem proving methods and
automated theorem proving methods.

In the philosophy of Interactive Theorem Proving (ITP), the programmer
is responsible for the specification and the implementation of a
computer program, but he also expresses theorems and conducts the
corresponding proofs in a formal proof system. Interactive theorem
proving methods are closely tied to proof assistants (cf. \isahol{},
\coq{}, \textsf{PVS}, etc.), which offer the possibility to specify,
implement, perform proofs over a given program in the same framework.
The programmer builds the proof for a given theorem in an interactive
manner, for instance assisted by a \textit{tactic} language in the
case of the \coq{} proof assistant (see
Chapter~\ref{chap:prelim-notions} for an example). Each proof
assistant comes with its own specification language and underlying
proof system. In between the world of interactive theorem proving and
automatic theorem proving, we can also cite the
\textit{contract-based} verification methods based on the Floyd-Hoare
logic \cite{Floyd1993} and Dijkstra's weakest precondition calculus
\cite{Dijkstra1976} such as SPARK \cite{Carre1988}, ESC
\cite{Rustan1998}, the B-method \cite{Abrial1991}, Frama-C
\cite{Kirchner2015}, or the Escher Verifier \cite{Carlton2013}.

The Automated Theorem Proving (ATP) methods aim at automating the
deduction steps involve in the proof search. Multiple automated
theorem provers exist based on different proof search techniques such
as natural deduction (e.g. \isa{} \cite{Nipkow2002}), the tableaux
method (e.g. the FaCT++ reasoner \cite{Tsarkov2006}, Zenon
\cite{Bonichon2007}), or resolution algorithms (e.g. the E theorem
prover \cite{Schulz2002}, Vampire
\cite{Riazanov2001}).\\

In this thesis, we address the problem of the formal verification of a
particular program. This program transforms an input model, which is
an instance of a particular kind of Petri nets (PNs), into a program
written in a Hardware Description Language (HDL). The program, the
context in which it is involved, the specificities of the input model,
and the target HDL, will all thoroughly be presented in this
thesis. Here, we want to zoom in on the nature of the considered
program that we aim to formally verified. The transformation from an
instance of one formalism to another instance of another formalism is
analog to the case of a \textit{compiler} program. The only difference
is that here an input to the transformation is not a program of a
source \textit{language}, but rather a model of an abstract source
formalism, namely a PN model.  Thus, our formal verification task
amounts to the formal \textit{verification} of a compiler program.
The problem of compiler verification has greatly stimulated the use of
formal methods in the field of software verification. Because a
complete computer system is made out of complete chain of hardware,
firmware and software components, the ultimate goal of the
verification of such a system is to be able to prove the safety of all
the layer composing it. In this system of layers, the place of
compiler programs are mandatory as they are placed at the layer
interfaces. Indeed, one can prove that a given program and a given
hardware is safe, but what if the compilation phase from the given
program to low-level version introduces errors and behavior
divergences. With these considerations in mind, compiler verification
is an important aspect for one that needs to certify an entire
computer system.

Thus, certifying a compiler program amounts to proving that the
compiler verifies certain properties; \cite{Patrignani2019} presents
three of them. First, one can verify that a compiler is
\textit{type-preserving}, also called the \textit{subject reduction}
property. A type-preserving compiler yields a well-typed target
program given a well-typed source program. Second, one can verify that
a compiler is \textit{semantic-preserving}. A semantic-preserving
compiler yields a target program that behaves similarly to the source
program. Thirdly, one can verify that the compiler is
\textit{equivalence-preserving}. Given two source programs that verify
a certain \textit{source-level} equivalence relation, an
equivalence-preserving compiler yields two corresponding target
programs that verify a certain \textit{target-level} equivalence
relation; this target-level equivalence relation is of course somehow
related to the source-level equivalence relation. In this thesis, we
are interested in proving that a \textit{compiler-like} transformation
program is \textit{semantic-preserving}.

\cite{Leroy2009} lists several techniques that exist to establish that
a compiler is semantic-preserving.

The first method is simply called \textit{compiler
  verification}. Compiler verification aims at establishing the
semantic-preserving property of a compiler program by proving a
so-called semantic preservation theorem of the form:

\begin{center}
  For all source program $S$, and compiler $C$ from the language of
  $S$ to a target language, $S$ has the behavior $B$ (written
  $S\Downarrow{}B$) iff $C(S)$
  (i.e. the compiled version of $S$) has the behavior $B$:\\

  $\forall{}S,C,B$,
  $S\Downarrow{}B\Leftrightarrow{}C(S)\Downarrow{}B$.
  
\end{center}

Now the above form of the theorem is the strongest one, i.e. it can be
proved only for a very particular kind of source and target
languages. Other refined versions of this semantic preservation
theorem exist depending on the nature of the source and target
languages. Proving such a theorem is often performed with the help of
a proof assistant as can be witnessed in the pioneering work on the
\ccert{} compiler \cite{Leroy2006}; thus, compiler verification falls
under the hood of deductive verification methods. 

The second method is called compiler \textit{validation}. Compiler
validation does not aim at proving a theorem stating that a given
compiler program is semantic preserving for all input programs. The
strategy of compiler validation is to equip the compiler program with
a \textit{validator} program. Each time the compiler produces a target
program from a source program, the validator tries afterwards to prove
that the two programs have the same behavior. To establish such a
proof, the validator program often relies on model-checking or
abstract interpretation techniques.

The third method is called \textit{proof-carrying} compilation. In
this setting, the compiler program generates alongside the target
program a proof that this program conforms to some property. The
generated proof must be is such a format that can be verified by a
\textit{proof-checker}, built in a proof assistant for instance.

Even though it is not considered as a formal method, compiler testing
is also a way to validate the semantic preservation property for a
given compiler program. Considering the essential part that plays
compiler programs regarding the production of software products, a lot
of efforts has been dedicated for the generation of test suites
\cite{Chen2020}.

In the thesis, we will follow the \textit{compiler verification}
technique. Consequently, our aim is to prove a semantic preservation
theorem over a transformation program and mechanize the process within
the framework of the \coq{} proof assistant.

\section{The \hilecop{} methodology}
\label{sec:hilecop}

In this section, we present in more details the context of our work,
and more specifically, the subject of our verification task,
i.e. \hilecop{}, a methodology for the design and the production of
safety-critical digital
systems.  % In Section~\ref{sec:design-crit-digit-systms}, we
% motivate the use of Model-Based Systems Engineering (MBSE) and formal
% methods in the design and production of safety-critical digital
% systems; in Section~\ref{sec:intro-hilecop}, we give an overall
% presentation of the \hilecop{} methodology, which applies both the
% principles of MBSE and formal methods; in
% Section~\ref{sec:verif-hilecop}, we point out the specific
% \textit{transformation} phase we propose to verify; we also lay out
% the intended strategy to verify this transformation phase and the
% corresponding research questions.

\subsection{Designing safety-critical digital systems}
\label{sec:design-crit-digit-systms}

According to Moore's law \cite{Moore2006}, the complexity of digital
integrated circuits is always increasing. To give an example, the
cut-of-the-edge \emph{AMD Epyc Rome} microprocessor (2019) is made out
of 50 billion of transistors. Composing billions of transistors on a
wired circuit is no more a task for humans but is very suited to
computers. However, engineers need to think about the design of
digital circuits in a way that is understandable for
humans. Therefore, they need high-level views of the circuits they are
designing in order to work together and to communicate about the
designs. The domain of Model-Based Systems Engineering (MBSE)
\cite{Long2011} proposes a framework to help engineers to design and
produce digital circuits, in a well-documented, safe and reliable
way. Comparable to what Model Driven Engineering (MDE) does in the
world of software engineering, models are first order concepts in
MBSE. A model represents a simplified view of real object. As
illustrated in Figure~\ref{fig:MBSE-ps}, a MBSE process describes a
way to design a digital circuit starting from a high-level view of the
system. This high-level view can follow a graphical formalism such as
SysML \cite{Friedenthal2014} or Petri nets \cite{Petri1962}, or a
textual one such as SystemC \cite{Black2009} or VHDL
\cite{Ashenden2010}. Then, the MBSE process describes many refinements
phases (the downward-going green arrows in Figure~\ref{fig:MBSE-ps})
during which the input model will be transformed; at each refinement
phase, the model goes down in abstraction towards its final
implementation as a hardware circuit. A refinement phase, which is
also a transformation phase, can be performed automatically, manually,
or both. Depending on the refinement phase, the full automation of the
transformation can sometimes never be achieved. In that case, a manual
intervention is necessary.

\begin{figure}[H]
  \centering
  \includegraphics[keepaspectratio,width=.7\textwidth]{Figures/Hilecop/MBSE-ps}
  \caption[A Model-Based Systems Engineering process.]{A Model-Based
    Systems Engineering process. Level 1 represents the highest
    abstraction level while Level n represents the concrete
    implementation of the system.  REQ stands for requirements, BEH
    for behavior, ARCH for architecture, Dgn V\&V for design
    verification and validation. This figure is an excerpt from
    \cite{Long2011}.}
  \label{fig:MBSE-ps}
\end{figure}

In the case where the digital circuit being designed is a
safety-critical system, an MBSE process will often employ formal
models, i.e. models with a formal mathematical definition, as the
design formalism.  Thus, these models enable a certain extent of
mathematical reasoning to prove that safety properties are met during
the design V\&V phase (cf. Figure~\ref{fig:MBSE-ps}).

The refinement process of the MBSE is really close to the one of the
B-method \cite{Abrial1991} for the development of safety-critical
software programs. The B-method allows the developers to specify,
implement and verify a software program at an abstract level, using
the B language; then, several refinement phases are performed until a
concrete program is generated in the Ada or C language.

\subsection{Introducing the \hilecop{} methodology}
\label{sec:intro-hilecop}

The INRIA CAMIN team has developed a new technology of neuroprostheses
\cite{Guiraud2006}. Neuroprostheses are medical devices which purpose
is to electro-stimulate the nerves of patients suffering from moving
disabilities. The nerves are responding to the stimulation, i.e. an
electric influx, in order to activate the muscles and so that the
patient can recover some movements. Thus, controlling stimulation
applied to the patient's nerve is a critical point of the device
overall functioning. This stimulation is generated and controlled by
an implanted mixed circuit (resp. analogous and digital parts),
embedded in the neuroprosthesis. Therefore, the design of such digital
systems becomes utterly critical as a faulty circuit could result in
the injury of patients. To assist the engineers in the design and the
implementation of these safety-critical digital systems, the CAMIN
team came up with a process called the ``\hilecop{} methodology''
\cite{Andreu2009}.  This methodology follows the principles of a MBSE
process and relies on several transformations going from abstract
models to concrete FPGA (Field-Programmable Gate Array) or ASIC
(Application-Specific Integrated Circuit) implementations through the
production of \vhdl{} code. Figure~\ref{fig:hilecop-wf} details the
global workflow of \hilecop{}.

\begin{figure}[H]
\centering
\includegraphics[keepaspectratio=true,width=\textwidth]{Figures/Hilecop/hilecop-wf}
\caption[Workflow of the \hilecop{} methodology.]{Workflow of the
  \hilecop{} methodology; horizontal double arrows indicate the
  transformation phases, i.e. the refinement phases in MBSE terms;
  simple arrows indicate different kinds of operations performed at a
  given step.}
\label{fig:hilecop-wf}
\end{figure}

In Figure~\ref{fig:hilecop-wf}, Step~1 corresponds to the design phase
of a digital system. At this step, the engineers produce a model of
the required system; the leveraged model formalism is a graphical
formalism specially designed for the methodology and based on
component diagrams. Figure~\ref{fig:components-and-pn} provides an
example of such a model.
%
\begin{figure}[H]
\centering
\includegraphics[keepaspectratio=true,width=\textwidth] {Figures/Hilecop/abs-model}
\caption[An example of \hilecop{} high-level model.]{An Example of
  \hilecop{} high-level model. Black diamonds represent \vhdl{}
  signals.}
\label{fig:components-and-pn}
\end{figure}
%
As shown in Figure~\ref{fig:components-and-pn}, a component of the
\hilecop{} high-level model formalism is represented by a box having
an internal behavior and an interface that allows the connection to
other components. The internal behavior of a component is defined with
a specific kind of Petri Net (PN) model. These PNs and their
distinguishing features will be thoroughly presented in
Chapter~\ref{chap:hilecop-models}. The component interface exposes
references to the places, transitions, and signals of the internal
behavior to the outside so that multiple components can be
assembled. Each component has a clock and a reset input port
(\texttt{clk} and \texttt{rst}) in its interface. The presence of the
\texttt{clk} port shows that the \hilecop{} methodology has been built
for the design of synchronous digital systems. To a certain extent,
\vhdl{} signals can be integrated to the high-level components to
represent a direct wiring between components. A component behavior can
also be defined through the composition of other components. In that
case, we talk about a composite hardware structure.

Next, in Figure~\ref{fig:hilecop-wf}, the transformation from Step~1
to Step~2 flattens the model. The internal behaviors are connected
according to the interface compositions, and embedding component
structures are removed. Figure~\ref{fig:impl-model} gives the result
of the flattening phase for the model of
Figure~\ref{fig:components-and-pn}. In Figure~\ref{fig:impl-model}, we
do not show the \vhdl{} signals that were present in
Figure~\ref{fig:components-and-pn}. As these signals already
constitute plain \vhdl{} code, they will simply be copied as they
stand during the model-to-text transformation happening from Step 2 to
Step 3.
%
\begin{figure}[H]
\centering
\includegraphics[keepaspectratio=true,width=\textwidth] {Figures/Hilecop/impl-model}
\caption[Global Petri net model.]{A global Petri net model obtained
  after the flattening of a \hilecop{} high level model.}
\label{fig:impl-model}
\end{figure}
%
The PN models used in \hilecop{} have been specifically devised for
the design of safety-critical digital systems; a first thesis has
formalized the execution semantics of these PN models
\cite{Leroux2014}. What makes them a very particular kind of models is
their \textit{synchronous} execution semantics. This semantics denotes
from the standard \textit{asynchronous} execution of PNs.  The PN
formalism is a formal model and therefore allows us to apply
mathematical reasoning on its instances. Particularly, a PN model can
be analyzed, and a proof that a given model meet some properties can
be automatically produced through the direct analysis of the structure
or through the use of model-checking techniques. This feature of PNs
has been one of the reason of the adoption of this formalism as
\hilecop{}'s base formalism. A thesis has been dedicated to the
development of new methods to analyze the \hilecop{} PN models
\cite{Merzoug2018}.  In fact, the transformation of the abstract model
is a bit different in preparation of the model analysis. The
transformation adds new information to the flattened model to help the
analysis. Figure~\ref{fig:impl-model} only gives the flattened version
of the model produced in preparation of the next transformation into a
\vhdl{} design.  The analysis phase is here to convince the engineers
that they are indeed designing a safe system. The analysis process is
a round trip between Step~1 and Step~2.  It aims at producing a model
that is conflict-free (see Section~\ref{sec:sitpn-wd} for more
details about the definition of a conflict), bounded, and
deadlock-free, using model-checking techniques.  After several
iterations, the model should reach soundness and is then said to be
\emph{implementation-ready}.

In Figure~\ref{fig:hilecop-wf}, from Step~2 to Step~3, \vhdl{} source
code is then generated by means of an automatic model-to-text
transformation. The generated code describes a \vhdl{} design, i.e. a
textual description of a hardware system, which has an interface
defining input and output ports and an internal behavior called an
architecture. Details about the syntax and the semantics of the
\vhdl{} language will be given in
Chapter~\ref{chap:hvhdl}. Figure~\ref{fig:vhdl-gen} succinctly
illustrates the transformation between Step~2 and Step~3. Each place
(resp. transition) of the input PN model (on the left) is transformed
into a \textit{place} (resp. \textit{transition}) component instance,
which is an instance of the \texttt{place} (resp. \texttt{transition})
design. % The \texttt{place} and \texttt{transition} designs define two
% ``classes'' of circuits (interface and internal behavior) that can be
% instantiated. The \vhdl{} code defining these two designs has been
% previously set by the creators of the \hilecop{} methodology
% \cite{Leroux2014}. These designs appear as \textit{static} code in
% Figure~\ref{fig:vhdl-gen}.
The transformation from Step~2 to Step~3
will be thoroughly presented in Chapter~\ref{chap:transformation}.
%
\begin{figure}[H]
\centering
\includegraphics[keepaspectratio=true,width=.8\textwidth] {Figures/Hilecop/vhdl-generation}
\caption[Generation of a \vhdl{} design from a Petri net.]{Generation
  of a top-level \vhdl{} design from a Petri net. On the left, the
  input PN and on the right, the generated \vhdl{} top-level
  design. Dotted arrows show the relation between the component
  instances and their source design.}
\label{fig:vhdl-gen}
\end{figure}
%
For the purpose of the \hilecop{} methodology, two \vhdl{} designs
have been defined: the \texttt{place} design, which is a hardware
description of a PN place (circle nodes in a PN) and the
\texttt{transition} design, which is a hardware description of a PN
transition (square nodes in a PN). Like all \vhdl{} designs, the
\texttt{place} and the \texttt{transition} designs have an input and
output port interface, and their own internal behavior. A \vhdl{}
design describes a kind of ``class'' of hardware component. Thus, a
design can be instantiated in the behavior of other designs in order
to obtain more complex behaviors. As illustrated in
Figure~\ref{fig:vhdl-gen}, the transformation from Step~2 to Step~3
creates a place component (or design) instance (PCI) and a transition
component (or design) instance (TCI) for each place and transition of
the input Petri net. Then, the PCIs and TCIs are connected together
through their input and output port interfaces. These connections
reflect the arc connections, and thus the interactions, between the
places and the transitions of the input PN model.

From Step~3 to Step~4, the \vhdl{} compilation/synthesis and the FPGA
programming, or ASIC realization, are finally performed using
industrial tools. At the end of Step~4, the designed circuit is
physically built on an FPGA device or an ASIC.  What happens between
Step~3 and Step~4 appears as a black box in the whole \hilecop{}
methodology. Therefore, we will not consider this transformation
phase, which will not be verified.

\subsection{Verifying the \hilecop{} methodology}
\label{sec:verif-hilecop}

% The \hilecop{} methodology is useful to design and implement
% safety-critical digital systems.
The use of Petri nets as a base model is one of the major advantage of
the \hilecop{} methodology. All the analysis tools that accompany the
Petri net formalism, and allow us to prove that the models meet some
required properties, qualify the \hilecop{} methodology as a formal
method for the design and implementation of safety-critical digital
systems. However, even with input models that are proved to be
\textit{sound}, the advantages provided by the use Petri nets would be
lost if one of the transformation performed during the process changes
the input definition of the circuit in a way that would alter its
behavior. Thus, the engineers would have specified a perfectly correct
digital system but would never obtain the expected circuit on a
physical device. Therefore, in order to reinforce the confidence in
the \hilecop{} methodology, the goal of this thesis is to verify, by
establishing a formal proof, that the model-to-text transformation
from Step~2 to Step~3 (i.e. the framed part with red dotted lines in
Figure~\ref{fig:hilecop-wf}) preserves the behavior of the input
models into the generated \vhdl{} designs. We choose to carry out this
task as a deductive verification task.  We aim at proving a theorem
stating that the \hilecop{} model-to-text transformation is
\textit{semantic-preserving}. This theorem will be of the following
form: for all PN model, input to the \hilecop{} transformation, the
generated output \vhdl{} design behaves similarly at execution
time. Chapter~\ref{chap:proof} formally presents our behavior
preservation theorem, and thus, what we mean about the similarity of
execution between a PN model and a \vhdl{} design.

One could argue that to qualify the entire \hilecop{} methodology, one
has to verify all the transformations used in the methodology,
i.e. consider also the transformation from Step~1 to Step~2, and the
transformation from Step~3 to Step~4. However, we shall say that:

\begin{itemize}
\item The transformation from Step~1 to Step~2 changes the structure
  of the component-based input model. Even if the removal of the
  component structures induces some structural rearrangements, the
  behavior of the flattened model is almost similar to the one of the
  component-based model. Therefore, we argue that verifying that this
  transformation is semantic-preser\-ving is an easy enough task.

\item The transformation from Step~3 to Step~4 is performed by
  industrial tools. We rely on these tools because they are widely
  used in the industry for the development of safety-critical systems
  (e.g. cadence tools in aerospace and defense domains). Moreover, the
  compiler/synthesizer used at this stage of the methodology is a
  proprietary product. Thus, we don't have any access to the code of
  this program. Moreover, the compiler/synthesizer performs a lot of
  optimizations over the input \vhdl{} code. Even with a provided
  access to the code, verifying such an optimizing compiler would not
  possible within the time-span of this thesis.
\end{itemize}

Now that we have clarified the nature of the verification task we want
to achieve, we can state our research question as follows:

\begin{center}
  \textsc{ Can we prove that the model-to-text transformation
    described in the \hilecop{} methodology is semantic preserving?}
\end{center}

This task is really close to the formal verification of compilers for
programming languages. Compiler verification has been widely explored,
and many works are accessible in the literature \cite{Dave2003}. The
major source of inspiration of this thesis has been the work done on
the \ccert{} certified C compiler \cite{Leroy2009}. Thus, we argue
here that the scientific interest of our research comes from the
comparison between the methods used to perform our verification task
and the methods used to perform similar verification task in other
domains such as compiler verification. Thus, we can complement our
research question with the following ones:

\begin{itemize}
\item What are the similarities and the differences between the
  \hilecop{} transformation and other transformation situations
  (compilers, model transformations\dots)?
\item Is there a strategy to perform the verification of the
  \hilecop{} transformation?
\item How far the correspondence holds between this strategy and the
  strategy used in other transformation situations such as compiler
  verification?
\end{itemize}

To achieve the formal verification of \hilecop{}, our approach is
similar to what has been done for the \ccert{} compiler. The idea is
to formalize the semantics of the source and target languages, and
verify that the transformation preserves the semantics of any input
model. In the thesis, we propose both to perform the formalization
work on ``paper'' and mechanize it within the \coq{} proof assistant
\cite{Bertot2004}.

In the case of \hilecop{}, some specificities of the source and target
languages introduce additional technical difficulties in the process
of formal verification. A first difference pertains to \hilecop{}'s
high-level formalism (the input language), which is quite
abstract. This formalism depends on PNs, and thus is not a common
programming language.

A second difference is about the \vhdl{} language (the output
language).  Similarly to the PN models used in \hilecop{}, the \vhdl{}
language is not a common programming language as its purpose is both
the structural and behavioral description of hardware
circuits. % Although previous work has been conducted toward the
% formalization of the \vhdl{} semantics \cite{Kloos2012}, a semantics
% that is able to both handle all the constructs in the generated
% programs, and facilitate the proof of behavior preservation, still
% needs to be designed.

To further motivate the necessity of the verification task, the
development of neuroprostheses by the INRIA CAMIN team is at the base
of the creation of the Neurrinov
company\footnote{\url{http://neurinnov.com/}}. The Neurrinov company
is now looking towards the industrial development of such
neuroprostheses. We hope that once the verification performed on the
\hilecop{} methodology, it will help to obtain the CE certification,
related to the EU 2017/745 regulation text, necessary to qualify the
neuroprostheses as eligible for the medical market.

Moreover, the \hilecop{} methodology comes with a working
implementation based on the Eclipse framework. This software is
currently used by the engineers of the Neurinnov company to design the
digital systems having a part in the
neuroprostheses. Figure~\ref{fig:hilecop-soft} gives a view of the
existing \hilecop{} software.

\begin{landscape}
  \begin{figure}[H]
    \centering
    \includegraphics[keepaspectratio=true,width=\linewidth]{Figures/Hilecop/Hilecop2.png}
    \caption[A view of the \hilecop{} software.]{A view of the
      \hilecop{} software implemented on top of the Eclipse
      framework. The middle frame shows high-level model of digital
      system such as can be designed in the \hilecop{} methodology. On
      the right side, the frames correspond to the palette of tools
      available to the user to build a model of a digital system.}
    \label{fig:hilecop-soft}
  \end{figure}
\end{landscape}

To the purpose of formal verification, we will implement the
\hilecop{} model-to-text transformation leveraging the functional
language of the \coq{} proof assistant. However, after the
mechanization of the proof of semantic preservation, we could use the
extraction feature of the \coq{} proof assistant to produce the
implemented transformation as an \ocaml{} program. Then, we will be
able to connect this program to the existing \hilecop{} software in
order
to use the verified version of the transformation.\\

This thesis memoir is structured as follows.

Chapter~\ref{chap:prelim-notions} introduces all the necessary
mathematical notions to understand the remainder of the memoir.
Chapter~\ref{chap:hilecop-models} presents in an informal and formal
way a specific kind of Petri net models; these models are the input to
the \hilecop{} transformation program.  Chapter~\ref{chap:hvhdl} gives
an informal presentation of the \vhdl{} language. The \vhdl{} language
is the target language in which the programs generated by the
\hilecop{} transformation are written. We also give in this chapter a
formal definition of the syntax and semantics of a subset of the
\vhdl{} language that we call \hvhdl{}.
Chapter~\ref{chap:transformation} presents the algorithm of the
\hilecop{} transformation and its implementation with the \coq{} proof
assistant.  Chapter~\ref{chap:proof} details the semantic preservation
theorem expressing that the \hilecop{} transformation is
semantic-preserving.  It also gives the high-level theorems and lemmas
involved in the proof of the semantic preservation theorem.  Finally,
Chapter~\ref{chap:concl} ends the memoir, and outlines the
perspectives regarding the full completion of the task of proving that
the \hilecop{} transformation is semantic-preserving.

The results of a literature review pertaining to the formal semantics
\vhdl{} is presented at the beginning of
Chapter~\ref{chap:hvhdl}. Similarly, the results of a literature
review pertaining to the task of compiler verification in the world of
deductive verification methods are presented at the beginning of
Chapters~\ref{chap:transformation} and \ref{chap:proof}.

% All the programming tasks of this thesis have been performed within
% the framework of the \coq{} proof assistant. The produced code, of
% which some fragments are presented all along this thesis memoir, is
% fully accessible under the following \textsf{Git} repository:

% \begin{center}
%   \url{https://github.com/viampietro/ver-hilecop}
% \end{center}



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
