% Chapter Template

\chapter{Introduction}
\label{chap:intro}

With the use of every human-bred technology is associated a
risk. Regarding the nature of the technology, and the broader system
in which it is involved, the consequences of a failure can be
dramatic. Thus arises the notion of the safety of systems;
\cite{Bowen1993} gives the following definition of safety:

\begin{center}
  ``Safety can [\dots] be defined as the freedom from exposure to
  danger, or the exemption from hurt, injury or loss.''
\end{center}

A \textit{safety-critical} system can be understood as a system for
which the safety aspect is the main concern, being that important
consequences, such as direct human losses, natural catastrophes, or
economic disasters, could result of the failure of the system. In this
thesis, conducted in the field of computer science, we are
particularly interested in safety-critical \textit{computer}
systems. The concept of computer system encompasses both the low-level
hardware-related and the more abstract software-related aspects
involved in computer technologies. These days, computers pervade a
considerable number of objects and technologies that pave our
every-day life, including safety-critical systems. Thus, the risk
associated with the use of computers in certain critical applications
is real. Failures of safety-critical computer systems have happened
and continue to happen; the list of critical incidents maintained by
the ACM Committee on Computers and Public Policy and Peter G. Neumann
ever since the mid 80s \cite{Neumann1994} is always
growing\footnote{The \emph{risks digest} website continues to register
  the computer-related incidents that resulted or could result in
  important damages: \url{https://catless.ncl.ac.uk/Risks/} }.

To ensure the safety of computer systems involved in critical domains
such as avionics, railway, power plants or medicine, there exists a
number of standards and norms developed by international
organizations. These standards set of a number of rules and techniques
to be followed for the design, the production and the validation of
safety-critical computer systems. To cite some well-known standards,
the \href{https://www.eurocae.net/}{EUROCAE} and
\href{https://www.rtca.org/}{RTCA} organisms has devised the
ED-12C/DO-178C and ED-80/DO-254 industry standards for the development
cycle of software and hardware computer systems involved in avionics;
the \href{https://www.cencenelec.eu/}{CENELEC} has defined the
EN-50128 standard for the development of software programs for railway
control and protection systems; the \href{https://www.iec.ch}{IEC} is
at the origin of the IEC-60880 standard for the development of
software programs involved in the control of power plants; the USA,
Canada and the EU have defined the
\href{https://www.commoncriteriaportal.org/}{Common Criteria (CC)}
referential for the evaluation of the safety of systems and software
programs. In this thesis, we are interested in verifying a
\textit{computer-related} methodology involved in the production of
safety-critical medical devices. This domain is regulated by the EU
2017/145
standard\footnote{\url{http://data.europa.eu/eli/reg/2017/745/2020-04-24}}
that sets the rules for the development of medical devices, including
how to validate the technologies involved in the production line.

The rules imposed by the standards vary with respect to the
criticality of the considered systems; for instance, in the medical
field, the regulation text 2017/145 of the EU, pertaining to the
marketing of medical devices, sets a different requirement level
whether we are considering the production of dressings (level 0), or
of neuroprotheses (highest-level, level 4). The IEC defines a SIL
(Safety Integrity Level) measure that qualifies the criticality level
of a system. The CC defines a level of Evaluation Assurance Level
(EAL), from 1 to 7, that must be met by the evaluated systems
regarding their functional requirements.

Among the mandatory procedures, prescribed by the standards, which
must be followed to validate a computer system involved in a
safety-critical system, there are tests (unit, functional or
integration tests) or simulation (especially applied to hardware
systems) are to be noted.  However, in the case of the development
safety-critical computer systems, a particular kind of methods, called
\textit{formal methods} (FM), are also applied. In formal methods, a
computer system is considered as a mathematical object
\cite{Bjorner2014}. As pointed out in \cite{Bowen1993}, ``formal
methods address \textit{correctness} issues'', that is whether a
system delivers the required service. The perks of formal methods are
to set a formal mathematical framework around a computer system.  This
framework allows us to reason about the system and prove that the
system meets some required properties. Thus, a ``formal methods''
framework for computer systems requires at least a formal requirement
language, i.e. with a formal semantics, to express the properties that
a given system must verify. The expression of these required
properties is called the \textit{specification} of the system.  We can
cite some specification formalism such as CCS \cite{Milner1980}, CSP
\cite{Hoare1978}, Petri nets \cite{Petri1962} or TLA+
\cite{Lamport1994} to describe reactive systems (i.e. systems that
continuously interact with an environment); hardware description
languages such as \vhdl{} \cite{Lipsett1986} and \textsf{Verilog}
\cite{IEEE1996} can also be considered as formal specification
languages for hardware designs if provided with a formal semantics and
embedded in a formal proof system (see for instance
\cite{Borrione1992}). We can also cite specification languages such as
VDM \cite{Bjorner1987a}, the Z notation \cite{Abrial2013} on which is
based the B language \cite{Abrial1991} (included in the broader
B-method); but also, all the theorem provers and proof assistants that
come with their own specification languages such as \isahol{}
\cite{Nipkow2002}, \coq{} \cite{Coq2021}, \textsf{PVS}
\cite{Crow1995}, etc.  A FM framework must also provide a formal proof
system to reason about the formal specification of the system. Some FM
frameworks come with means to implement the computer system or
simplified version of the system (i.e. a model) in a formal
setting. This latter kind of framework enables to check if the
implementation of a system always complies with its specification,
i.e. the \textit{soundness} of the system, and if all the aspects of
the specification are met by the implementation, i.e. the
\textit{completeness} of the system.

Even though the purpose is always to check the correctness of systems,
there exist multiple kinds of formal verification techniques. These
techniques can be separated in three groups. The first group refers to
the \textit{deductive verification} methods; the techniques aims at
establishing some proofs over a computer system in a formal proof
setting; the deduction process can either be \textit{interactive}
(i.e. conducted by a human) or automated.  The second group refers to
model-checking techniques. The third group refers to the abstract
interpretation of programs. The techniques applied to the formal
verification of hardware computer systems or software computer systems
are quite similar.  Thus, most of the techniques presented here apply
to both hardware and software developments. Note that the following
presentation of formal verification techniques is not exhaustive.

\paragraph{Abstract interpretation}

Abstract interpretation \cite{Cousot1977} aims at performing automatic
static analysis of programs by approximating the possible execution
states of the program. To do so, the concrete domains of the program
variables are related to more abstract domains notably through the use
of a lattice structure. Afterwards, invariant properties that the
program verifies can be automatically checked against the lattice
structure. For instance, abstract interpretation can help to determine
that a given program terminates. To give examples of
\textit{abstract-interpretation-based} tools, we can cite the
TERMINATOR static analyzer \cite{Cook2006} that proves termination and
other properties over C programs, or the Astr√©e \cite{Blanchet2002}
program analyzer for real-time embedded systems.

Similarly to abstract interpretation, symbolic execution
\cite{Boyer1975} is another method for the static analysis of programs
based on the partial execution of the considered programs. The method
consists in generating all the execution traces, also called
\textit{symbolic} traces, of a given program; the result takes the
form of a symbolic execution tree. In these traces, some of the
program inputs, i.e. the \textit{variables}, will be associated with
\textit{symbolic} expressions denoting the fact that the values of
these inputs are yet unknown. Thus, by reasoning on the definition
domains of these inputs, some properties of the program can be checked
at execution points, that is, at the nodes of the symbolic execution
tree. The property checking process is most of the time performed by a
constraint solver \cite{Amadini2020}, where constraints are expressed
over the \textit{symbolic} variables of the program. Pertaining to the
construction of the symbolic execution tree, it is obtained most of
the time by applying the rules of a structural formal semantics
\cite{Plotkin2004} associated with the language of the considered
program. Typically, a branching in the execution tree is the result of
the evaluation of a conditional statement. Each path of the execution
tree is associated with a satisfiability condition, i.e. a Boolean
formula, that determines if a given execution point is reachable or
not. Symbolic execution methods are often used to generate test suites
with an important coverage of the execution paths of programs. To give
examples of \textit{symbolic-execution-based} tools, we can cite the
DART \cite{Godefroid2005} or CUTE \cite{Sen2006} test generators.

\paragraph{Model checking}
Model-checking techniques \cite{Queille1982, Clarke1986} build a model
reflecting the execution of a computer system by enumerating all the
possible execution states of the system. Then, the \textit{execution}
model can be automatically checked against some properties that the
computer system must verify. The execution model must be a
finite-state model, i.e. the enumeration of the execution states of
the model must not be infinite.  Most of the time, the properties that
the computer system must verify are expressed through formulas of a
modal logic.  Model-checking techniques are broadly used for the
formal verification of reactive systems, especially hardware
systems. In that case, the properties that must be met by the
considered system are expressed within formulas of one of the many
temporal logics (Interval Temporal Logic \cite{Moszkowski1985}, Linear
Temporal Logic \cite{Pnueli1977}, Computation Tree Logic
\cite{Emerson1984}, etc.), which are handy to express time-related
properties. To give examples of well-known model-checkers, we can cite
the BLAST \cite{Henzinger2002}, CADP \cite{Garavel2013} or
\textsc{Uppaal} \cite{Bengtsson1996} model-checkers.

\paragraph{Deductive software verification}
``Deductive software verification aims at formally verifying that all
possible behaviors of a given program satisfy formally defined,
possibly complex properties, where the verification process is based
on some form of logical inference, i.e., `deduction' ''
\cite{Hahnle2019}.  Deductive software verification methods are
divided into two categories: interactive theorem proving methods and
automated theorem proving methods.

In the philosophy of Interactive Theorem Proving (ITP), the programmer
is responsible for the specification and the implementation of a
computer program, but he also expresses theorems and conducts the
corresponding proofs in a formal proof system. Interactive theorem
proving methods are closely tied to proof assistants (cf. \isahol{},
\coq{}, \textsf{PVS}, etc.), which offer the possibility to specify,
implement, perform proofs over a given program in the same framework.
The programmer builds the proof for a given theorem in an interactive
manner, for instance assisted by a \textit{tactic} language in the
case of the \coq{} proof assistant (see
Chapter~\ref{chap:prelim-notions} for an example). Each proof
assistant comes with its own specification language and underlying
proof system. In between the world of interactive theorem proving and
automatic theorem proving, we can also cite the
\textit{contract-based} verification methods based on the Floyd-Hoare
logic \cite{Floyd1993} and Dijkstra's weakest precondition calculus
\cite{Dijkstra1976} such as SPARK \cite{Carre1988}, ESC
\cite{Rustan1998}, the B-method \cite{Abrial1991}, Frama-C
\cite{Kirchner2015}, or the Escher Verifier \cite{Carlton2013}.

The Automated Theorem Proving (ATP) methods aim at automating the
deduction steps involve in the proof search. Multiple automated
theorem provers exist based on different proof search techniques such
as natural deduction (e.g. \isa{} \cite{Nipkow2002}), the tableaux
method (e.g. the FaCT++ reasoner \cite{Tsarkov2006}, Zenon
\cite{Bonichon2007}), or resolution algorithms (e.g. the E theorem
prover \cite{Schulz2002}, Vampire
\cite{Riazanov2001}). \\

In this thesis, we address the problem of the formal verification of a
particular program. This program transforms an input model, which is
an instance of a particular kind of Petri nets (PNs), into a program
written in a Hardware Description Language (HDL). The program, the
context in which it is involved, the specificities of the input model,
and the target HDL, will all thoroughly be presented in this
thesis. Here, we want to zoom in on the nature of the considered
program that we aim to formally verified. The transformation from an
instance of one formalism to another instance of another formalism is
analog to the case of a \textit{compiler} program. The only difference
is that here an input to the transformation is not a program of a
source \textit{language}, but rather a model of an abstract source
formalism, namely a PN model.  Thus, our formal verification task
amounts to the formal \textit{verification} of a compiler program.
The problem of compiler verification has greatly stimulated the use of
formal methods in the field of software verification. Because a
complete computer system is made out of complete chain of hardware,
firmware and software components, the ultimate goal of the
verification of such a system is to be able to prove the safety of all
the layer composing it. In this system of layers, the place of
compiler programs are mandatory as they are placed at the layer
interfaces. Indeed, one can prove that a given program and a given
hardware is safe, but what if the compilation phase from the given
program to low-level version introduces errors and behavior
divergences. With these considerations in mind, compiler verification
is an important aspect for one that needs to certify an entire
computer system.

Thus, certifying a compiler program amounts to proving that the
compiler verifies certain properties; \cite{Patrignani2019} presents
three of them. First, one can verify that a compiler is
\textit{type-preserving}, also called the \textit{subject reduction}
property. A type-preserving compiler yields a well-typed target
program given a well-typed source program. Second, one can verify that
a compiler is \textit{semantic-preserving}. A semantic-preserving
compiler yields a target program that behaves similarly to the source
program. Thirdly, one can verify that the compiler is
\textit{equivalence-preserving}. Given two source programs that verify
a certain \textit{source-level} equivalence relation, an
equivalence-preserving compiler yields two corresponding target
programs that verify a certain \textit{target-level} equivalence
relation; this target-level equivalence relation is of course somehow
related to the source-level equivalence relation. In this thesis, we
are interested in proving that a \textit{compiler-like} transformation
program is \textit{semantic-preserving}.

\cite{Leroy2009} lists several techniques that exist to establish that
a compiler is semantic-preserving.

The first method is simply called \textit{compiler
  verification}. Compiler verification aims at establishing the
semantic-preserving property of a compiler program by proving a
so-called semantic preservation theorem of the form:

\begin{center}
  For all source program $S$, and compiler $C$ from the language of
  $S$ to a target language, $S$ has the behavior $B$ (written
  $S\Downarrow{}B$) iff $C(S)$
  (i.e. the compiled version of $S$) has the behavior $B$:\\

  $\forall{}S,C,B$,
  $S\Downarrow{}B\Leftrightarrow{}C(S)\Downarrow{}B$.
  
\end{center}

Now the above form of the theorem is the strongest one, i.e. it can be
proved only for a very particular kind of source and target
languages. Other refined versions of this semantic preservation
theorem exist depending on the nature of the source and target
languages. Proving such a theorem is often performed with the help of
a proof assistant as can be witnessed in the pioneering work on the
\ccert{} compiler \cite{Leroy2006}; thus, compiler verification falls
under the hood of deductive verification methods. 

The second method is called compiler \textit{validation}. Compiler
validation does not aim at proving a theorem stating that a given
compiler program is semantic preserving for all input programs. The
strategy of compiler validation is to equip the compiler program with
a \textit{validator} program. Each time the compiler produces a target
program from a source program, the validator tries afterwards to prove
that the two programs have the same behavior. To establish such a
proof, the validator program often relies on model-checking or
abstract interpretation techniques.

The third method is called \textit{proof-carrying} compilation. In
this setting, the compiler program generates alongside the target
program a proof that this program conforms to some property. The
generated proof must be is such a format that can be verified by a
\textit{proof-checker}, built in a proof assistant for instance.

Even though it is not considered as a formal method, compiler testing
is also a way to validate the semantic preservation property for a
given compiler program. Considering the essential part that plays
compiler programs regarding the production of softwares, a lot of
efforts has been dedicated for the generation of test suites
\cite{Chen2020}.

In the thesis, we will follow the \textit{compiler verification}
technique. Consequently, our aim is to prove a semantic preservation
theorem over a transformation program and mechanize the process within
the framework of the \coq{} proof assistant.

This thesis memoir is structured as follows.

Chapter~\ref{chap:prelim-notions} introduces all the necessary
mathematical notions to understand the remainder of the memoir.
Chapter~\ref{chap:hilecop} presents the context of our work, namely
the \hilecop{} methodology, which describes a process to design and
produce safety-critical digital systems. This chapter zooms in on the
part of the methodology that we propose to formally verify; it also
exposes our research question(s) with respect to this task.
Chapter~\ref{chap:hilecop-models} presents in an informal and formal
way a specific kind of Petri net models; these models are the input to
the \hilecop{} transformation program.  Chapter~\ref{chap:hvhdl} gives
an informal presentation of the \vhdl{} language. The \vhdl{} language
is the target language in which the programs generated by the
\hilecop{} transformation are written. We also give in this chapter a
formal definition of the syntax and semantics of a subset of the
\vhdl{} language that we call \hvhdl{}.
Chapter~\ref{chap:transformation} presents the algorithm of the
\hilecop{} transformation and its implementation with the \coq{} proof
assistant.  Chapter~\ref{chap:proof} details the semantic preservation
theorem expressing that the \hilecop{} transformation is
semantic-preserving.  It also gives the high-level theorems and lemmas
involved in the proof of the semantic preservation theorem.  Finally,
Chapter~\ref{chap:concl} ends the memoir, and outlines the
perspectives regarding the full completion of the task of proving that
the \hilecop{} transformation is semantic-preserving.

The results of a literature review pertaining to the formal semantics
\vhdl{} is presented at the beginning of
Chapter~\ref{chap:hvhdl}. Similarly, the results of a literature
review pertaining to the task of compiler verification in the world of
deductive verification methods are presented at the beginning of
Chapters~\ref{chap:transformation} and \ref{chap:proof}.

% All the programming tasks of this thesis have been performed within
% the framework of the \coq{} proof assistant. The produced code, of
% which some fragments are presented all along this thesis memoir, is
% fully accessible under the following \textsf{Git} repository:

% \begin{center}
%   \url{https://github.com/viampietro/ver-hilecop}
% \end{center}



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
