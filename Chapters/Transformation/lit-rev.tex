In this section, we present our literature review pertaining to
transformation functions in the context of formal verification. Here,
a transformation function is understood as any kind of mapping from a
source representation to a target representation, where the source and
target representations possess a behavior of their own (i.e. they are
executable). We use the same articles to perform our literature review
in this chapter and in the following chapter,
i.e. Chapter~\ref{chap:proof}. However, our research questions,
i.e. the questions we try to give an answer to while reading the
articles, and our presentation axis differ from one chapter to the
other.  Here, the following questions guide our reading:

\begin{itemize}
\item Is there a proper way to build a transformation function? Are
  there standards depending on the application domain?
\item How can we build a modular, extensible transformation function?
\item How can we build a transformation function that will ease the
  proof of semantic preservation?
\end{itemize}

The goal is to inspire ourselves with the works of the literature, and
to see how far the correspondence holds between our specific case of
transformation, and other cases of transformations. % The results of the
% literature review are presented in two parts. The two parts have
% been prepared based on the same material. The first part will be
% focusing on the expression of the transformation functions in the
% literature, and the second part will be focusing on the proof that
% these transformations are semantic preserving ones.
The material we used for the literature review is divided in three
categories. Each category covers a specific case of transformation
function. The three categories are:

\begin{itemize}
\item Compilers for generic programming languages
\item Compilers for hardware description languages
\item Model-to-model and model-to-text transformations
\end{itemize}

Note that, in the case of compilers for programming languages, the
term \textit{translation} is preferred over transformation to talk
about the generation of a target program from a source program.

\subsection{Building transformation functions}
\label{sec:build-transf}

As the authors state in \cite{Tan2016}, ``Although theoretically
possible, verifying a compiler that is not designed for verification
would be a prohibitive amount of work in practice.'' The question is
to know how to design such a compiler? How to anticipate the fact that
we will have to prove that the compiler is semantic preserving? Now,
let us consider these questions in the more general context of
transformation functions that map a source representation to a target
one.

\paragraph{Compilers for generic programming languages} In the context
of formally verified compilers for generic programming languages, the
translation from a source program to a target program is straight
forward. While descending recursively through the AST of the input
program, each construct of the source language is mapped to one or
many constructs of the target
language. Figure~\ref{fig:java-expr-to-java-bytecode} gives an example
of the translation from \java{} program expressions to \java{}
bytecode expressions, set in the context of a compiler for \java{}
programs written within the \isahol{} theorem prover
\cite{Strecker2002}. Here, the mapping between source and target
constructs is clearly defined.
\begin{figure}[H]
  \centering
  \includegraphics[keepaspectratio,width=.6\linewidth]{Figures/Proof/java-exprs-transl}
  \caption{Translation from \java{} expressions to \java{} bytecode expressions}
  \label{fig:java-expr-to-java-bytecode}
\end{figure}
In the works pertaining to the well-known \ccert{} project
\cite{Leroy2009, Blazy2006}, the many steps that compose the compiler
from C programs to assembly programs are also clearly mapping each
construct of source program to target program constructs.  Moreover,
the pattern matching possibilities offered by languages like \coq{},
\isa{}, \hol{} and other interactive theorem provers enable a clear
and concise implementation of compilers.

The cases of optimizing compilers like \cite{Leroy2009} and \cite{Tan}
show that, to avoid writing too complex functions when passing from a
source to a target program, the compilation is decomposed into many
passes. No more than 12 passes for the CakeML compiler, and up to 7
passes for \ccert{}. This is a way to keep the translation functions
simple enough in order to ease reasoning afterwards. Indeed, the more
the gap is important between the source representation and the target
one, the more the translation function will be complex.

Another point that is noticeable while expressing a translation
function is the necessity to keep a binding between the source and the
target representations. For instance, in \ccert, when passing from
transformed C programs to an RTL representation (based on registers
and control flow graphs), a binding function $\gamma$ links the
variables of a C program to the registers generated in the RTL
representation of the program. The binding is necessary for both the
translation and the proof of semantic preservation. During the
translation, it permits to replace the variables by their
corresponding registers in the RTL code. During the proof of semantic
preservation, the link that exists between a variable and a register
indicates which elements must be compared to prove that the execution
state of the source representation is similar to the execution state
of the target representation. The generation of this binding function
must be integrated to the design of the translation function.

In \cite{Leroy2009}, and \cite{Chlipala2010}, compilers are written
within the \coq{} proof assistant. Compilers are expressed using the
state-and-error monad, thus mimicking the traits of imperative
languages into a functional programming language setting. In
Section~\ref{sec:trans-alg}, we present the \hilecop{} transformation
in the form of an imperative pseudo-code algorithm. The
state-and-error monad is well-suited to the implementation of this
kind of algorithm with a functional language like \coq{}; thus, we
chose to apply this monad to our implementation of the transformation
algorithm (see Section~\ref{sec:trans-coq-impl}).

\paragraph{Compilers for hardware description languages}

The other category of compilers that we are interested in are
compilers for hardware description languages (HDL). The \hilecop{}
methodology's goal is the design of hardware circuits. For that
reason, we are interested in studying the case of compilers for
HDLs. However, one can notice that compiling an HDL program into a
lower level representation is one level of abstraction down compared
to the transformation we propose to verify. Indeed, it corresponds to
Step~3 in the \hilecop{} methodology
(cf. Figure~\ref{fig:hilecop-wf}), i.e. the transformation of VHDL
source code into an RTL representation.

In the context of formal verification applied to HDLs compilers, only
a few works describe the specificities of their translation function.

In \cite{Braibant2013}, the authors define the FeSi language (a
refinement of the BlueSpec language, a specification language for
hardware circuit behaviors), and its implementation within the \coq{}
proof assistant.  The authors present the syntax and semantics of the
FeSi language and of the RTL language which is the target language of
the compiler.  FeSi programs are composed of simple expressions, and
actions permitting to read or write from different types of memory
(registers). Therefore, the abstract syntax is divided into the
definition of expressions and the definition of actions, i.e: control
flow instructions and operations on memory. The RTL language is
composed of expressions and write operations to registers. The authors
are more interested in proving that a FeSi specification is
well-implemented by a given \coq{} program, than giving the details of
the translation from FeSi to RTL. However, the translation seems
straight-forward, and proceeds as usual by descending through the AST
of FeSi programs.

In \cite{Bourgeat2020}, the authors present a compiler for the
language Koîka, which is also a simpler version of the BlueSpec
language. A Koîka program is composed of a list of rules; each rule
describes actions that must be performed atomically. Actions are read
and write operations on registers. A Koîka program is accompanied by a
scheduler that specify an execution order for the rules. The described
compiler transforms Koîka programs into RTL descriptions of hardware
circuits. The translation function builds an RTL circuit by
descending recursively down the AST of rules. Each action is
translated into a specific RTL representation which are afterwards
composed together to get complex circuits. The translation becomes
trickier when it comes to decide the composition of RTL circuits to
respect the execution order prescribed by the scheduler.

In \cite{Bourke}, the authors present the verification of a compiler
toolchain from \textsf{Lustre} programs to an imperative language
(Obc), and from Obc to Clight.  The Clight target is the one defined
in \ccert{} \cite{Leroy2009}.  \textsf{Lustre} permits the definition
of programs composed of nodes that are executed synchronously.  Nodes
treat input streams and yield output streams of values.  A node body
is composed of sequence of equations that determine the values of
output streams based on the input.  Obc programs are composed of class
declarations. A class declaration has a vector of memory variables, a
vector of instances of other classes, and method declarations.  The
translation turns each node of a \textsf{Lustre} program into a class
of Obc accompanied by two methods: the reset method, for the
initialization of the streams, and the step method, for the update of
values resulting of a synchronous step.

In \cite{Loow2021}, the authors describe a compiler that transforms
Verilog programs into netlists targeting given FPGA models. Verilog
programs are a lot like VHDL programs; they describe a hardware
circuit behavior in terms of processes. A netlist is composed of
registers, variables and a list of cells corresponding to
combinational components. During the translation process, the
expressions of the Verilog programs are turned into netlist cells, and
the composition of statements leads to the creation of complex
circuits by means of cell composition.

\paragraph{Model transformations}

We will now present the works pertaining to model-to-model and
model-to-text transformations in the context of formal
verification. Because of the nature of the transformation we propose
to verify, i.e a model-to-text transformation, the following works are
of particular interest to us. We will focus here on the manner to
express transformations in the case of model-to-model and
model-to-text transformations. Also, we tried to find articles related
to model transformations involving Petri nets.

In \cite{Berramla2015}, the authors observe that Model-Driven
Engineering (MDE) is all about model transformation operations. They
propose to set a formal context within the \coq{} proof assistant to
verify that model transformations preserve the structure of the source
models into the target models. To illustrate their methodology, they
choose to transform UML state machine diagrams into Petri net
models. The translation rules from source to target models are
expressed within the setting of the OMG standard QVT language
(Query/View/Transform). The QVT language offers a formal way to
express model transformations, partly based on the Object Constraint
Language (OCL). The translation rules map the different kind of
structures that can be found in UML state diagrams to specific
structures of Petri nets. Even though the two models used as source
and target of transformations are executable, the authors leverage the
formal context provided by \coq{} to prove that the expressed
transformations preserve certain structural properties.

In \cite{Calegari2011}, the authors describe a process for model
transformation where transformation rules are expressed with the Atlas
Transformation Language (ATL). Transformation rules in ATL involve
both declarative (OCL) and imperative (match rules) instructions. The
authors show how the ATL rules can easily be translated into \coq{}
relations. An example is given on the kind of model-to-model
transformations that can be implemented that way. The example is a UML
class diagram to relational database model transformation.

In \cite{Combemale2009}, the authors explore the different ways to
give a formal semantics to a Domain-Specific Language (DSL) in the
context of MDE. Here, the syntax of a given DSL is expressed with a
meta-model.  An instantiation of this meta-model (a model) yields a
DSL program. The authors specify a transformation from a DSL model to
another executable model, thus providing an \textit{translational}
semantics to the DSL model.  The authors illustrate their approach
with a source DSL named xSPEM, which is a process description
language. The target models are timed PNs. The transformation is
expressed through a structural mapping; i.e, each element of an xSPEM
model is mapped to a particular PN: an activity is mapped to a subnet,
a resource to a single place, connection from activity to resource
through parameter is mapped to a connection of transitions and places
in the resulting PN\dots 

In \cite{Dyck2019}, the authors address the problem of expressing
model transformations by using transformation graphs. Precisely, the
kind of transformation graphs that are used are called Triple Graph
Grammar (TGG). A TGG is a triplet ${<}s,c,t{>}$ where the
``correspondence model $c$ explicitly stores correspondence
relationships between source model $s$ and target model $t$''.

The work described is \cite{Fronc2011} is really close to our own
verification task. The article describes how Coloured Petri Nets
(CPNs, specifically LLVM-labelled Petri nets) are transformed into
LLVM (Low Level Virtual Machine) programs representing the state space
(the graph of reachable markings) of these PNs. LLVM is a low-level
assembly language. The aim is to enable an efficient model-checking of
the CPNs.  LLVM-labelled PNs are CPNs whose places, transitions and
arcs have LLVM constructs for color domains. Places are labelled with
data types.  Transitions are labelled with boolean expressions that
correspond to the guard of the transition. Arcs are labelled by
multisets of expressions. A marking is a function that maps each place
to a multiset of values belonging to the place's type.  The authors
define data structures (multisets, sets, markings\dots) with
interfaces, i.e. sets of operations over structures, to represent the
Petri nets in LLVM.  They define interpretation functions that draw
equivalences between Petri nets objects and LLVM data structures.  The
authors define two algorithms: $\mathtt{fire\_t}$ and
$\mathtt{succ\_t}$ to compute the graph of reachable states.  These
are the functions that transform CPNs into concrete LLVM programs.

In \cite{Meghzili2017}, the author describes a transformation from UML
state machine diagrams to Coloured Petri Nets (CPNs). The aim is to
leverage the means of analysis provided by Petri nets to certify
certain properties over UML state machine diagrams. The authors want
to verify that the transformation preserve structural properties
between source and target models. The transformation function does not
use a standard setting as QVT or ATL, or transformation graphs. It is
expressed as a specific function written in \isahol.

In \cite{Yang2014}, the author presents a transformation from
Architecture Analysis and Design Language (AADL) models to Timed
Abstract State Machines (TASMs). AADL is a language widely used in
avionics to describe both hardware and software systems. AADL doesn't
have a lot of tools to analyze and simulate the designed systems;
therefore transforming AADL models into TASM enables the use of an
important toolbox for analysis, and simulation. The transformation
from AADL to TASMs are described with ATL rules.

\paragraph{Discussions on how to build transformation functions in the
  context of semantic preservation}

Transformation functions are mappings from a source representation to
a target representation. The more the mapping from source to target is
straight-forward the easier the comparison will be when proving that
the transformation is semantic preserving. Thus, in
\cite{Leroy2009,Tan,Chlipala2010} where complex case of optimizing
compilers are presented, the compilation is split into many simple
passes to ease the verification effort coming afterwards. In the case
of the \hilecop{} transformation, we are not yet concerned with the
optimization of the generated \vhdl{} code. Thus, our transformation
algorithm performs the generation of the target \hvhdl{} design in a
single pass. We do not need to use intermediary representations
between the input SITPN model and the generated \hvhdl{} design.

Also, while transforming source programs, the compiler must often
generate fresh constructs belonging to the target language (for
instance, generating a fresh RTL register for each variable referenced
in a source C program in \cite{Leroy2009}). The compiler must keep a
binding, that is, a memory of the mapping between the elements of the
source program and their mirror in the target program. This
consideration is of interest in our case of transformation where the
elements of SITPNs are also mirrored by elements in the generated
\hvhdl{} design.

It remains hard to establish a standard way to express a
transformation function as it really depends on the form of the input
and the output representations. Compilers for programming languages
tend to be a lot more compositional than model transformations. Here,
the word \textit{compositional} means that the translation rules can
be split into simple and independent cases of translation, e.g.
translation of expressions, then translation of statements, then
translation of function bodies,\dots This is a huge advantage to
perform the proof of semantic preservation. Indeed, this decomposition
of a translation function permits to reason on simple translation
cases; yet, each of these translations cases yields a piece of target
code that can be executed or interpreted in an independent manner. In
the case of the \hilecop{}, we tried as much as possible to express
the transformation in a compositional way. First, we tried to devise
the transformation by building up transformation functions for each
element of the SITPN structure, i.e.: a transformation function for
the places, another for the transitions\dots However, due to the
interconnections that exist between the component instances of the
generated \hvhdl{} design, it is impossible to define transformation
functions that would yield stand-alone executable code.

In the world of models, there exist some standard formalisms to
express transformation rules (QVT, ATL, transformation graphs\dots).
However, the complexity of the transformation rules depends on the
richness of the elements composing the source model, and the distance
to the concepts of the target model. In our case, we were not able to
grab the perks of using such formalisms as QVT or ATL to devise our
transformation.




%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../main"
%%% End:
